import os
import asyncio
import time
import pysrt
import streamlit as st
import edge_tts
import subprocess # Th∆∞ vi·ªán ƒë·ªÉ ch·∫°y l·ªánh h·ªá th·ªëng nh∆∞ FFmpeg

# --- C√°c h√†m x·ª≠ l√Ω c·ªët l√µi ---

async def convert_text_to_speech(text, voice, rate, volume, pitch):
    """H√†m chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n ƒë∆°n gi·∫£n (kh√¥ng thay ƒë·ªïi)."""
    communicate = edge_tts.Communicate(text, voice, rate=rate, volume=volume, pitch=pitch)
    temp_file = f"temp_output_{int(time.time())}.mp3"
    await communicate.save(temp_file)
    return temp_file

async def convert_srt_to_timed_speech_with_ffmpeg(srt_content, voice, rate, volume, pitch):
    """
    H√†m chuy·ªÉn ƒë·ªïi SRT ƒë√£ ƒë∆∞·ª£c vi·∫øt l·∫°i ho√†n to√†n ƒë·ªÉ s·ª≠ d·ª•ng tr·ª±c ti·∫øp FFmpeg,
    lo·∫°i b·ªè s·ª± ph·ª• thu·ªôc v√†o pydub v√† pyaudioop.
    """
    subs = pysrt.from_string(srt_content)
    
    temp_files = [] # Danh s√°ch c√°c file t·∫°m ƒë·ªÉ d·ªçn d·∫πp sau n√†y
    concat_list_path = "filelist.txt"
    last_end_time_ms = 0
    
    progress_bar = st.progress(0, text="B·∫Øt ƒë·∫ßu x·ª≠ l√Ω ph·ª• ƒë·ªÅ...")

    for i, sub in enumerate(subs):
        start_time_ms = sub.start.ordinal
        
        # 1. T·∫°o kho·∫£ng l·∫∑ng b·∫±ng FFmpeg
        silence_duration_ms = start_time_ms - last_end_time_ms
        if silence_duration_ms > 50: # Ch·ªâ t·∫°o kho·∫£ng l·∫∑ng n·∫øu n√≥ ƒë·ªß l·ªõn
            silence_file = f"temp_silence_{i}.mp3"
            duration_sec = silence_duration_ms / 1000.0
            # L·ªánh FFmpeg ƒë·ªÉ t·∫°o file mp3 ch·ª©a s·ª± im l·∫∑ng
            cmd = [
                'ffmpeg', '-f', 'lavfi', '-i', 'anullsrc=r=24000', 
                '-t', str(duration_sec), '-q:a', '3', silence_file
            ]
            subprocess.run(cmd, check=True, capture_output=True, text=True)
            temp_files.append(silence_file)

        # 2. T·∫°o file √¢m thanh cho c√¢u tho·∫°i
        if sub.text.strip():
            speech_file = f"temp_speech_{i}.mp3"
            communicate = edge_tts.Communicate(sub.text, voice, rate=rate, volume=volume, pitch=pitch)
            await communicate.save(speech_file)
            temp_files.append(speech_file)
        
        last_end_time_ms = sub.end.ordinal
        progress_bar.progress((i + 1) / len(subs), text=f"ƒê√£ x·ª≠ l√Ω ph·ª• ƒë·ªÅ {i+1}/{len(subs)}")

    # 3. T·∫°o file danh s√°ch ƒë·ªÉ FFmpeg n·ªëi c√°c file l·∫°i
    with open(concat_list_path, "w", encoding='utf-8') as f:
        for filename in temp_files:
            f.write(f"file '{os.path.abspath(filename)}'\n")

    # 4. Ch·∫°y l·ªánh FFmpeg ƒë·ªÉ n·ªëi t·∫•t c·∫£ c√°c file
    final_output_path = f"timed_output_{int(time.time())}.mp3"
    concat_cmd = [
        'ffmpeg', '-f', 'concat', '-safe', '0', 
        '-i', concat_list_path, '-c', 'copy', final_output_path
    ]
    subprocess.run(concat_cmd, check=True, capture_output=True, text=True)

    # 5. D·ªçn d·∫πp t·∫•t c·∫£ c√°c file t·∫°m
    for f in temp_files:
        if os.path.exists(f): os.remove(f)
    if os.path.exists(concat_list_path): os.remove(concat_list_path)
    
    progress_bar.empty()
    return final_output_path


# --- Giao di·ªán ng∆∞·ªùi d√πng Streamlit ---

st.set_page_config(layout="wide")
st.markdown("<h1 style='text-align: center; color: #3498db;'>C√îNG C·ª§ TTS BY L√ù VƒÇN HI·ªÜP</h1>", unsafe_allow_html=True)
st.markdown("---")

# *** ƒê√É C·∫¨P NH·∫¨T: Th√™m l·∫°i danh s√°ch gi·ªçng ƒë·ªçc ƒë·∫ßy ƒë·ªß ***
all_voices = [
    # Vietnamese
    { "ShortName": "vi-VN-HoaiMyNeural", "FriendlyName": "HoaiMy", "Locale": "Vietnamese", "Gender": "Female" },
    { "ShortName": "vi-VN-NamMinhNeural", "FriendlyName": "NamMinh", "Locale": "Vietnamese", "Gender": "Male" },
    # Korean
    { "ShortName": "ko-KR-BongJinNeural", "FriendlyName": "BongJin", "Locale": "Korean", "Gender": "Male" },
    { "ShortName": "ko-KR-GookMinNeural", "FriendlyName": "GookMin", "Locale": "Korean", "Gender": "Male" },
    { "ShortName": "ko-KR-InJoonNeural", "FriendlyName": "InJoon", "Locale": "Korean", "Gender": "Male" },
    { "ShortName": "ko-KR-JiMinNeural", "FriendlyName": "JiMin", "Locale": "Korean", "Gender": "Female" },
    { "ShortName": "ko-KR-SeoHyeonNeural", "FriendlyName": "SeoHyeon", "Locale": "Korean", "Gender": "Female" },
    { "ShortName": "ko-KR-SunHiNeural", "FriendlyName": "SunHi", "Locale": "Korean", "Gender": "Female" },
    # Japanese
    { "ShortName": "ja-JP-AoiNeural", "FriendlyName": "Aoi", "Locale": "Japanese", "Gender": "Female" },
    { "ShortName": "ja-JP-DaichiNeural", "FriendlyName": "Daichi", "Locale": "Japanese", "Gender": "Male" },
    { "ShortName": "ja-JP-KeitaNeural", "FriendlyName": "Keita", "Locale": "Japanese", "Gender": "Male" },
    { "ShortName": "ja-JP-NanamiNeural", "FriendlyName": "Nanami", "Locale": "Japanese", "Gender": "Female" },
    # US English
    { "ShortName": "en-US-BrandonNeural", "FriendlyName": "Brandon", "Locale": "English (US)", "Gender": "Male" },
    { "ShortName": "en-US-ChristopherNeural", "FriendlyName": "Christopher", "Locale": "English (US)", "Gender": "Male" },
    { "ShortName": "en-US-CoraNeural", "FriendlyName": "Cora", "Locale": "English (US)", "Gender": "Female" },
    { "ShortName": "en-US-DavisNeural", "FriendlyName": "Davis", "Locale": "English (US)", "Gender": "Male" },
    { "ShortName": "en-US-ElizabethNeural", "FriendlyName": "Elizabeth", "Locale": "English (US)", "Gender": "Female" },
    { "ShortName": "en-US-EmmaNeural", "FriendlyName": "Emma", "Locale": "English (US)", "Gender": "Female" },
    { "ShortName": "en-US-EricNeural", "FriendlyName": "Eric", "Locale": "English (US)", "Gender": "Male" },
    { "ShortName": "en-US-GuyNeural", "FriendlyName": "Guy", "Locale": "English (US)", "Gender": "Male" },
    { "ShortName": "en-US-JacobNeural", "FriendlyName": "Jacob", "Locale": "English (US)", "Gender": "Male" },
    { "ShortName": "en-US-JaneNeural", "FriendlyName": "Jane", "Locale": "English (US)", "Gender": "Female" },
    { "ShortName": "en-US-JasonNeural", "FriendlyName": "Jason", "Locale": "English (US)", "Gender": "Male" },
    { "ShortName": "en-US-JennyNeural", "FriendlyName": "Jenny", "Locale": "English (US)", "Gender": "Female" },
    { "ShortName": "en-US-MichelleNeural", "FriendlyName": "Michelle", "Locale": "English (US)", "Gender": "Female" },
    { "ShortName": "en-US-MonicaNeural", "FriendlyName": "Monica", "Locale": "English (US)", "Gender": "Female" },
    { "ShortName": "en-US-NancyNeural", "FriendlyName": "Nancy", "Locale": "English (US)", "Gender": "Female" },
    { "ShortName": "en-US-RogerNeural", "FriendlyName": "Roger", "Locale": "English (US)", "Gender": "Male" },
    { "ShortName": "en-US-SaraNeural", "FriendlyName": "Sara", "Locale": "English (US)", "Gender": "Female" },
    { "ShortName": "en-US-SteffanNeural", "FriendlyName": "Steffan", "Locale": "English (US)", "Gender": "Male" },
    { "ShortName": "en-US-TonyNeural", "FriendlyName": "Tony", "Locale": "English (US)", "Gender": "Male" },
]

col1, col2 = st.columns([1, 2])
with col1:
    st.header("B·∫£ng ƒëi·ªÅu khi·ªÉn ‚öôÔ∏è")
    search_term = st.text_input("T√¨m ki·∫øm gi·ªçng ƒë·ªçc (e.g., vi, en-US, female...)", "")
    filtered_voices = [v for v in all_voices if search_term.lower() in str(v).lower()]
    voice_options = {f"{v['FriendlyName']} ({v['Locale']}, {v['Gender']})": v['ShortName'] for v in filtered_voices}
    
    # ƒê·∫£m b·∫£o index kh√¥ng b·ªã l·ªói n·∫øu kh√¥ng t√¨m th·∫•y gi·ªçng ƒë·ªçc n√†o
    selectbox_options = list(voice_options.keys())
    selected_voice_display = st.selectbox("Ch·ªçn gi·ªçng ƒë·ªçc", options=selectbox_options, index=0 if selectbox_options else -1)
    
    voice_select = voice_options.get(selected_voice_display) # L·∫•y gi√° tr·ªã an to√†n
    
    rate_val = st.slider("T·ªëc ƒë·ªô", -100, 100, 0)
    volume_val = st.slider("√Çm l∆∞·ª£ng", -100, 100, 0)
    pitch_val = st.slider("Cao ƒë·ªô (Hz)", -50, 50, 0)
    
    rate = f"{'+' if rate_val >= 0 else ''}{rate_val}%"
    volume = f"{'+' if volume_val >= 0 else ''}{volume_val}%"
    pitch = f"{'+' if pitch_val >= 0 else ''}{pitch_val}Hz"

with col2:
    st.header("N·ªôi dung & K·∫øt qu·∫£ üìù")
    uploaded_file = st.file_uploader("T·∫£i l√™n file .txt ho·∫∑c .srt", type=['txt', 'srt'])
    
    is_srt_timed = False
    if uploaded_file and uploaded_file.name.lower().endswith('.srt'):
        is_srt_timed = st.checkbox("ƒê·ªìng b·ªô h√≥a th·ªùi gian file SRT", value=True)

    st.divider()
    text_input = st.text_area("Ho·∫∑c nh·∫≠p vƒÉn b·∫£n v√†o ƒë√¢y...", height=250, value=uploaded_file.read().decode('utf-8') if uploaded_file else "")
    
    if st.button("Chuy·ªÉn ƒë·ªïi th√†nh gi·ªçng n√≥i", use_container_width=True, type="primary"):
        if not text_input.strip() or not voice_select:
            if not text_input.strip():
                st.error("Vui l√≤ng nh·∫≠p vƒÉn b·∫£n ho·∫∑c t·∫£i l√™n m·ªôt file.")
            if not voice_select:
                 st.error("Kh√¥ng c√≥ gi·ªçng ƒë·ªçc n√†o ƒë∆∞·ª£c ch·ªçn. Vui l√≤ng x√≥a b·ªô l·ªçc t√¨m ki·∫øm.")
        else:
            with st.spinner('ƒêang x·ª≠ l√Ω, vui l√≤ng ch·ªù...'):
                try:
                    output_file = None
                    if uploaded_file and uploaded_file.name.lower().endswith('.srt') and is_srt_timed:
                         srt_content = text_input
                         output_file = asyncio.run(convert_srt_to_timed_speech_with_ffmpeg(srt_content, voice_select, rate, volume, pitch))
                    else:
                        output_file = asyncio.run(convert_text_to_speech(text_input, voice_select, rate, volume, pitch))

                    st.success("Ho√†n t·∫•t! ‚úÖ")
                    with open(output_file, 'rb') as audio_file:
                        audio_bytes = audio_file.read()
                        st.audio(audio_bytes, format='audio/mpeg')
                        st.download_button(
                            label="T·∫£i xu·ªëng file MP3",
                            data=audio_bytes,
                            file_name=os.path.basename(output_file),
                            mime='audio/mpeg'
                        )
                    
                    if os.path.exists(output_file): os.remove(output_file)
                except Exception as e:
                    st.error(f"ƒê√£ x·∫£y ra l·ªói: {e}")